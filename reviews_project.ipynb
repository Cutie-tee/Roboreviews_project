{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+tZq1yQS9fzpLTJ1o7DpE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cutie-tee/Roboreviews_project/blob/main/reviews_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset consists of 3 files: 1429_1.csv\n",
        "Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\n",
        "Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AiGNuG9FxzRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kuAr6V4SpWt9",
        "outputId": "76381118-3f21-4946-bfed-b6909c884b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def safe_read_csv(file_path):\n",
        "    \"\"\"\n",
        "    Safely reads a CSV file by handling parsing issues.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(file_path, low_memory=False, on_bad_lines='skip', quotechar='\"', escapechar='\\\\')\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load datasets safely\n",
        "file1_data = safe_read_csv('1429_1.csv')\n",
        "file2_data = safe_read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv')\n",
        "file3_data = safe_read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')\n",
        "\n",
        "# Check if all files loaded successfully\n",
        "if file1_data is not None and file2_data is not None and file3_data is not None:\n",
        "    # Standardizing column names\n",
        "    file1_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "    file2_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "    file3_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "    # Align datasets to common columns\n",
        "    common_columns = list(set(file1_data.columns) & set(file2_data.columns) & set(file3_data.columns))\n",
        "\n",
        "    # Selecting only common columns\n",
        "    file1_data = file1_data[common_columns]\n",
        "    file2_data = file2_data[common_columns]\n",
        "    file3_data = file3_data[common_columns]\n",
        "\n",
        "    # Concatenate datasets\n",
        "    combined_data = pd.concat([file1_data, file2_data, file3_data], ignore_index=True)\n",
        "\n",
        "    # Dropping duplicates\n",
        "    combined_data.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Resetting index\n",
        "    combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Save cleaned dataset\n",
        "    combined_data.to_csv('combined_reviews_cleaned.csv', index=False)\n",
        "\n",
        "    # Display overview\n",
        "    print(\"Dataset successfully cleaned and saved.\")\n",
        "    print(combined_data.info())\n",
        "else:\n",
        "    print(\"One or more files could not be loaded.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egQJ111b07sn",
        "outputId": "9060a10f-fea0-4688-ab2b-6ae1ed5e6353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully cleaned and saved.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67351 entries, 0 to 67350\n",
            "Data columns (total 17 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   reviews.title        67332 non-null  object \n",
            " 1   reviews.sourceURLs   67351 non-null  object \n",
            " 2   asins                67349 non-null  object \n",
            " 3   reviews.text         67350 non-null  object \n",
            " 4   manufacturer         67351 non-null  object \n",
            " 5   reviews.date         67312 non-null  object \n",
            " 6   categories           67351 non-null  object \n",
            " 7   brand                67351 non-null  object \n",
            " 8   id                   67351 non-null  object \n",
            " 9   reviews.rating       67318 non-null  float64\n",
            " 10  keys                 67351 non-null  object \n",
            " 11  reviews.id           71 non-null     float64\n",
            " 12  reviews.dateSeen     67351 non-null  object \n",
            " 13  reviews.username     67339 non-null  object \n",
            " 14  reviews.doRecommend  54511 non-null  object \n",
            " 15  reviews.numHelpful   54605 non-null  float64\n",
            " 16  name                 60591 non-null  object \n",
            "dtypes: float64(3), object(14)\n",
            "memory usage: 8.7+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def debug_csv_in_chunks(file_path, chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Debug a CSV file in chunks to identify problematic rows.\n",
        "    \"\"\"\n",
        "    problematic_chunks = []\n",
        "    chunk_number = 0\n",
        "\n",
        "    try:\n",
        "        for chunk in pd.read_csv(\n",
        "            file_path, chunksize=chunk_size, low_memory=False, encoding='utf-8', on_bad_lines='skip'\n",
        "        ):\n",
        "            chunk_number += 1\n",
        "            # Check if the chunk is read successfully\n",
        "            chunk.head()  # Access the chunk to validate\n",
        "    except Exception as e:\n",
        "        problematic_chunks.append((chunk_number, str(e)))\n",
        "        print(f\"Error in chunk {chunk_number}: {e}\")\n",
        "\n",
        "    return problematic_chunks\n",
        "\n",
        "# Path to the problematic file\n",
        "problematic_file_path = 'Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv'\n",
        "\n",
        "# Run the debugging process\n",
        "problematic_chunks = debug_csv_in_chunks(problematic_file_path)\n",
        "print(\"Problematic chunks identified:\", problematic_chunks)\n",
        "\n",
        "def handle_bad_lines(bad_line):\n",
        "    \"\"\"\n",
        "    Custom error handler for bad lines in the CSV.\n",
        "    Attempts to fix string enclosure issues.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Assume the issue is with unescaped quotes within a string field\n",
        "        # This is a simple example, you may need more robust logic based on your data\n",
        "        bad_line = bad_line.replace('\"', '\\\\\"')\n",
        "        return bad_line\n",
        "    except Exception as e:\n",
        "        print(f\"Error handling bad line: {e}, Line: {bad_line}\")\n",
        "        return None  # Or choose to skip the line entirely\n",
        "\n",
        "\n",
        "# Read the file with skipping problematic lines and applying custom error handling\n",
        "\n",
        "cleaned_data = pd.read_csv(problematic_file_path, encoding='utf-8', on_bad_lines=handle_bad_lines, engine='python')\n",
        "\n",
        "# Save the cleaned file\n",
        "cleaned_data.to_csv('Datafiniti_Cleaned_May19.csv', index=False)\n",
        "print(\"Cleaned data saved as 'Datafiniti_Cleaned_May19.csv'\")\n"
      ],
      "metadata": {
        "id": "rGfEFrsF16CF",
        "outputId": "0bddefa7-7b21-4e3c-88c1-60b8d2b9c2ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problematic chunks identified: []\n",
            "Cleaned data saved as 'Datafiniti_Cleaned_May19.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Steps:\n",
        "Now that the problematic file has been cleaned and saved as Datafiniti_Cleaned_May19.csv, l\n",
        "\n",
        "1.  inspect the cleaned file to ensure it's ready for analysis:\n",
        "Check for missing values.\n",
        "Display an overview of the data.\n",
        "\n",
        "2. Merge Cleaned Data with Other Files\n",
        "Combine the cleaned file with the previously processed datasets to create a unified dataset.\n",
        "\n",
        "3. Sentiment Classification and Clustering\n"
      ],
      "metadata": {
        "id": "JK1HJxk22sJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r2HTjrCs07iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to datasets\n",
        "file1_path = '1429_1.csv'\n",
        "file2_path = 'Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'\n",
        "cleaned_file_path = 'Datafiniti_Cleaned_May19.csv'\n",
        "\n",
        "# Load datasets\n",
        "try:\n",
        "    file1_data = pd.read_csv(file1_path, encoding='utf-8', on_bad_lines='skip')\n",
        "    file2_data = pd.read_csv(file2_path, encoding='utf-8', on_bad_lines='skip')\n",
        "    cleaned_data = pd.read_csv(cleaned_file_path, encoding='utf-8')\n",
        "\n",
        "    # Standardize column names across datasets\n",
        "    file1_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "    file2_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "    cleaned_data.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "\n",
        "    # Find common columns for merging\n",
        "    common_columns = list(set(file1_data.columns) & set(file2_data.columns) & set(cleaned_data.columns))\n",
        "\n",
        "    # Select only common columns\n",
        "    file1_data = file1_data[common_columns]\n",
        "    file2_data = file2_data[common_columns]\n",
        "    cleaned_data = cleaned_data[common_columns]\n",
        "\n",
        "    # Concatenate datasets\n",
        "    combined_data = pd.concat([file1_data, file2_data, cleaned_data], ignore_index=True)\n",
        "\n",
        "    # Drop duplicates\n",
        "    combined_data.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Reset index\n",
        "    combined_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Save the merged dataset\n",
        "    combined_data.to_csv('Merged_Reviews_Dataset.csv', index=False)\n",
        "\n",
        "    print(\"Merged dataset saved as 'Merged_Reviews_Dataset.csv'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "AKDRlFQA208f",
        "outputId": "74c3667f-a95e-4fa0-ef3a-d3d2ca0c8186",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e4c09202db08>:10: DtypeWarning: Columns (1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  file1_data = pd.read_csv(file1_path, encoding='utf-8', on_bad_lines='skip')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged dataset saved as 'Merged_Reviews_Dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentinent analysis**"
      ],
      "metadata": {
        "id": "CdxvFw793L6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset with low_memory=False\n",
        "combined_data = pd.read_csv(\"Merged_Reviews_Dataset.csv\", low_memory=False)\n"
      ],
      "metadata": {
        "id": "hZph0lW75LKa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Goal is to classify customer reviews into three sentiment categories\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset with low_memory=False\n",
        "combined_data = pd.read_csv(\"Merged_Reviews_Dataset.csv\", low_memory=False)\n",
        "\n",
        "# Load sentiment analysis pipeline with GPU acceleration\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device=0\n",
        ")\n",
        "\n",
        "# Define a batch size for processing\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Drop missing reviews\n",
        "combined_data = combined_data.dropna(subset=['reviews.text'])\n",
        "\n",
        "# Function to truncate reviews to 512 tokens\n",
        "def truncate_text(text, max_length=512):\n",
        "    return text[:max_length]\n",
        "\n",
        "# Apply truncation to the review text\n",
        "combined_data['reviews.text'] = combined_data['reviews.text'].apply(lambda x: truncate_text(x, max_length=512))\n",
        "\n",
        "# Perform sentiment analysis in batches\n",
        "sentiments = []\n",
        "for i in tqdm(range(0, len(combined_data), BATCH_SIZE), desc=\"Processing Batches\"):\n",
        "    batch = combined_data['reviews.text'].iloc[i:i + BATCH_SIZE].tolist()\n",
        "    results = sentiment_analyzer(batch)\n",
        "    sentiments.extend([result['label'] for result in results])\n",
        "\n",
        "# Add sentiment results to the DataFrame\n",
        "combined_data['Sentiment'] = sentiments\n",
        "\n",
        "# Save the dataset with sentiment labels\n",
        "combined_data.to_csv(\"Dataset_with_Sentiment.csv\", index=False)\n",
        "print(\"Sentiment classification complete. Dataset saved as 'Dataset_with_Sentiment.csv'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t1srBWRh3P01",
        "outputId": "bfa32dfe-4740-4876-9e45-d1eee636176c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-6784edae03a2>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  combined_data['reviews.text'] = combined_data['reviews.text'].apply(lambda x: truncate_text(x, max_length=512))\n",
            "Processing Batches: 100%|██████████| 1053/1053 [06:28<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment classification complete. Dataset saved as 'Dataset_with_Sentiment.csv'\n"
          ]
        }
      ]
    }
  ]
}